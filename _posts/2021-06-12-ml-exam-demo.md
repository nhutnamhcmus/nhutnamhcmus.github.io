---
title: "Giáº£i Ä‘á» máº«u Nháº­p mÃ´n Há»c MÃ¡y"
categories:
  - Exam
  - Machine Learning
tags:
  - Exam
  - Machine Learning
toc: true
comments: true
header:
  teaser: "/assets/img/neural_ml.png"
---

Äá» thi máº«u mÃ´n Nháº­p mÃ´n Há»c MÃ¡y - K2018

# MÃ´ hÃ¬nh tuyáº¿n tÃ­nh

## CÃ¢u 01: Perceptron Learing Algorithm

Cho mÃ´ hÃ¬nh perceptron $y=sign(w_0 + w_1x)$ vá»›i $w_0 = 1$, $w_1 = 1$ vÃ  bá»™ dá»¯ liá»‡u kiá»ƒm tra $D = {(x_i, y_i)} = {(2, -1), (3, 1), (-2, -1), (-4, 1)}$. HÃ£y tÃ­nh **Ä‘á»™ chÃ­nh xÃ¡c** cá»§a mÃ´ hÃ¬nh?

**HÆ°á»›ng Ä‘i** Gá»i biáº¿n toÃ n cá»¥c correct = 0 lÃ  sá»‘ láº§n dá»± Ä‘oÃ¡n Ä‘Ãºng cá»§a mÃ´ hÃ¬nh

Vá»›i $x_1 = 2$, ta cÃ³ $y_{\text{pred}} = sign(1 + 2 \times 1) = 1 \ne y_1 = -1$

Vá»›i $x_2 = 3$, ta cÃ³ $y_{\text{pred}} = sign(1 + 3 \times 1) = 1 = y_2 = 1$, correct += 1

Vá»›i $x_3 = -2$, ta cÃ³ $y_{\text{pred}} = sign(1 + (-2) \times 1) = -1 = y_3 = -1$, correct += 1

Vá»›i $x_4 = -4$, ta cÃ³ $y_{\text{pred}} = sign(1 + (-4) \times 1) = -1 \ne y_4 = 1$

Äá»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh

$$\text{accuracy} = \frac{1}{n}\sum_{i=1}^{n}\left[sign(w_0 + w_1x_i) == y_i\right] = \frac{1}{4} \times 2 = 0.5$$


## CÃ¢u 02: Logistic Regression

Cho mÃ´ hÃ¬nh logistic regression:
$$y = \frac{1}{1+ exp(w_0 + w_1x)}$$ vá»›i $w_0 = 1$, $w_1 = 1$ vÃ  bá»™ dá»¯ liá»‡u kiá»ƒm tra $D = {(x_i, y_i)} = {(-3, -1), (-2, 1), (2, -1), (4, 1)}$. HÃ£y tÃ­nh **Ä‘á»™ lá»—i** cá»§a mÃ´ hÃ¬nh?

**HÆ°á»›ng Ä‘i** Gá»i biáº¿n toÃ n cá»¥c error = 0 lÃ  sá»‘ láº§n dá»± Ä‘oÃ¡n bá»‹ sai cá»§a mÃ´ hÃ¬nh

Vá»›i $x_1 = -3$, ta cÃ³ $y_{\text{pred}} = \left[\frac{1}{1+ exp(1 + 1(-3))} >= 0.5\right] = 1 \ne y_1 = -1$, error += 1

Vá»›i $x_2 = -2$, ta cÃ³ $y_{\text{pred}} = \left[\frac{1}{1+ exp(1 + 1(-2))} >= 0.5\right] = 1 = y_2 = 1$

Vá»›i $x_3 = 2$, ta cÃ³ $y_{\text{pred}} = \left[\frac{1}{1+ exp(1 + 1(2))} >= 0.5\right] = -1 = y_3 = -1$

Vá»›i $x_4 = 4$, ta cÃ³ $y_{\text{pred}} = \left[\frac{1}{1+ exp(1 + 1(4))} >= 0.5\right] = -1 \ne y_4 = 1$, error += 1

Äá»™ lá»—i cá»§a mÃ´ hÃ¬nh:

$$\text{error rate} = \frac{1}{n}\sum_{i=1}^{n}\left[\frac{1}{1+ exp(w_0 + w_1x_i)} \ne y_i\right] = \frac{1}{4} \times 2 = 0.5 = 1 - \text{accuracy}$$

## CÃ¢u 03: Linear Regression

Cho mÃ´ hÃ¬nh linear regression:
$$y = f(x) = w_0 + w_1x$$ vÃ  bá»™ dá»¯ liá»‡u D, hÃ£y xÃ¡c Ä‘á»‹nh mÃ´ hÃ¬nh vÃ  trá»±c quan mÃ´ hÃ¬nh

|  x 	|   y	|
|---	|---	|
|   1	|   2	|
|   2	|   1	|
|   4	|   2	|

$$ X = 
\begin{bmatrix}
    1       & 1\\
    1       & 2\\
    1       & 4
\end{bmatrix}
$$

$$ y =
\begin{bmatrix}
    2 \\
    1 \\
    2
\end{bmatrix}
$$

TÃ­nh ma tráº­n chuyá»ƒn vá»‹ cá»§a X 

$$ X^T = 
\begin{bmatrix}
    1 & 1 & 1\\
    1 & 2 & 4
\end{bmatrix}
$$

MÃ´ hÃ¬nh tá»‘t nháº¥t

$$W = \left(X^TX\right)^{-1}X^Ty$$

```python
import numpy as np 
import matplotlib.pyplot as plt 

xs = np.array([1, 2, 4])
ys = np.array([2, 1, 2])

one = np.ones(xs.shape[0])
X = np.stack((one, xs)).T

W = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), ys)
print(W)

[Output] [1.5        0.07142857]

x = np.linspace(1,4)
y_predict = W[0] + W[1]*x 
plt.scatter(xs, ys)
plt.plot(x, y_predict, c='red')
plt.show()
```

|![](/assets/images_posts/ml-testt.png)|

# MÃ´ hÃ¬nh cÃ¢y quyáº¿t Ä‘á»‹nh vÃ  thá»‘ng kÃª

Dá»¯ liá»‡u huáº¥n luyá»‡n báº£ng sau ð· cÃ³ 3 thuá»™c tÃ­nh Snow\_Dist, Weekend, Sun vÃ  má»™t thuá»™c tÃ­nh quyáº¿t
Ä‘á»‹nh Skiing. CÃ¢u 4, 5 vÃ  6 sáº½ sá»­ dá»¥ng dá»¯ liá»‡u nÃ y.

|  # 	|  Snow\_Dist 	|  Weekend 	|  Sun 	|  Skiing 	|
|---	|---	|---	|---	|---	|
|  1 	|  $\leq 100$ 	|  yes 	|  yes 	|  yes 	|
|  2 	|  $\leq 100$ 	|  yes 	|  yes 	|  yes 	|
|  3 	|  $\leq 100$ 	|  yes 	|  no 	|  yes 	|
|  4 	|  $\leq 100$ 	|   no	|  yes 	|  yes 	|
|  5 	|  $> 100$ 	|  yes 	|  yes 	|  yes 	|
|  6 	|  $> 100$ 	|  yes 	|  yes 	|  yes 	|
|  7 	|  $> 100$ 	|  yes 	|  yes 	|  no 	|
|  8 	|  $> 100$ 	|  yes 	|  no 	|  no 	|
|  9 	|  $> 100$ 	|  no 	|  yes 	|  no 	|
|  10 	| $> 100$  	|  no 	|  yes 	|  no 	|

## CÃ¢u 04: Decision Tree with Gini

TÃ¬m vÃ  váº½ táº¥t cáº£ cÃ¡c cÃ¢y stump sá»­ dá»¥ng Ä‘á»™ Ä‘o **gini** (khÃ´ng cáº§n cháº¡y tá»«ng bÆ°á»›c)

**HÆ°á»›ng Ä‘i** 

Ta cÃ³:

$$\text{Gini}(D) = 1 - \sum_{i=1}^{m}p_i^2$$

$$gini_{A}(D) = \frac{|D_1|}{|D|}gini(D_1)+\frac{|D_2|}{|D|}gini(D_2)$$

$$\Delta gini(A) = gini(D) - gini_A(D)$$

Tá»•ng sá»‘ bá»™: 10

Lá»›p P (Positive) = 6: Skiing = "yes"

Lá»›p N (Negative) = 4: Skiiing = "no"

$$gini(D) = 1 - \left(\frac{6}{10}\right)^2 - \left(\frac{4}{10}\right)^2 = 0.48$$

Xem xÃ©t thuá»™c tÃ­nh **Snow\_Dist**:{$\leq 100$, $> 100$}

$$gini_{\text{Snow_Dist}}(D) = \frac{|D_1|}{10}gini(D_1)+\frac{|D_2|}{10}gini(D_2)$$

$$= \frac{4}{10}\left(1-\left(\frac{4}{4}\right)^2\right) + \frac{6}{10}\left(1-\left(\frac{2}{6}\right)^2-\left(\frac{4}{6}\right)^2\right) = \frac{4}{15} = 0.267$$

Xem xÃ©t thuá»™c tÃ­nh **Weekend** {yes, no}

$$gini_{\text{Weekend}}(D) = \frac{|D_1|}{10}gini(D_1)+\frac{|D_2|}{10}gini(D_2)$$

$$= \frac{7}{10}\left(1-\left(\frac{5}{7}\right)^2-\left(\frac{2}{7}\right)^2\right) + \frac{3}{10}\left(1-\left(\frac{1}{3}\right)^2-\left(\frac{2}{3}\right)^2\right) = \frac{44}{105} = 0.41905$$

Xem xÃ©t thuá»™c tÃ­nh **Sun** {yes, no}

$$gini_{\text{Sun}}(D) = \frac{|D_1|}{10}gini(D_1)+\frac{|D_2|}{10}gini(D_2)$$

$$= \frac{8}{10}\left(1-\left(\frac{5}{8}\right)^2-\left(\frac{3}{8}\right)^2\right) + \frac{2}{10}\left(1-\left(\frac{1}{2}\right)^2-\left(\frac{1}{2}\right)^2\right) = \frac{19}{40} = 0.475$$

| Attribute  	|  Split 	|  Gini index 	|  Reduction in impurity $\Delta gini(A) = gini(D) - gini_A(D)$ 	|
|---	|---	|---	|---	|
| Snow_Dist  	| Binary  	|  0.267 	|  0.48 - 0.267 =  0.213	|
| Weekend  	|  Binary 	|  0.367 	| 0.48 - 0.41905 =  0.06095	|
|  Sun 	|  Binary 	|  0.3167 	|  0.48 - 0.475 = 0.005	|

Thuá»™c tÃ­nh Snow_Dist Ä‘Æ°á»£c chá»n vÃ¬ cÃ³ Gini index nhá» nháº¥t vÃ  Reduction in impurity lá»›n nháº¥t

## CÃ¢u 05: Decision Tree with Entropy

TÃ¬m vÃ  váº½ cÃ¢y quyáº¿t Ä‘á»‹nh sá»­ dá»¥ng Ä‘á»™ Ä‘o **entropy** (khÃ´ng cáº§n cháº¡y tá»«ng bÆ°á»›c)

**Äá»™ Ä‘o Entropy**

$$\text{Entropy}(S) = -\sum_{i=1}^{c}p_ilog_2p_i$$

**Average entropy** on attribute A

$$AE(S, A) = \sum_{v \in \text{Values}(A)}\frac{|S_v|}{|S|}\text{Entropy}(S_v)$$

**Information gain**

$$Gain(S, A) = Entropy(S) - AE(S, A)$$

Tá»•ng sá»‘ bá»™: 10

Lá»›p P (Positive) = 6: Skiing = "yes"

Lá»›p N (Negative) = 4: Skiiing = "no"

Ãp dá»¥ng:

$$Entropy(S) = -\frac{6}{10}log_2\left(\frac{6}{10}\right) -\frac{4}{10}log_2\left(\frac{4}{10}\right) = 0.97095$$

Xem xÃ©t thuá»™c tÃ­nh **Snow\_Dist**:{$\leq 100$, $> 100$}

- **Snow\_Dist** = $\leq 100$

$$Info([4, 0]) = Entropy([4, 0]) = -\frac{4}{4}log_2\left(\frac{4}{4}\right) -\frac{0}{4}log_2\left(\frac{0}{4}\right) = 0$$

- **Snow\_Dist** = $> 100$

$$Info([2, 4]) = Entropy([2, 4]) = -\frac{2}{6}log_2\left(\frac{2}{6}\right) -\frac{4}{6}log_2\left(\frac{4}{6}\right) = 0.918$$

- ThÃ´ng tin cá»§a thuá»™c tÃ­nh **Snow\_Dist**

$$Info([4, 0], [2, 4]) = \frac{4}{10} \times 0 + \frac{6}{10} \times 0.918 = 0.5508$$

- Äá»™ lá»£i thÃ´ng tin cá»§a thuá»™c tÃ­nh **Snow\_Dist**

$$Info([6, 4]) - Info([4, 0], 2, 4]) = Entropy(S) - Info([4, 0], 2, 4]) = 0.42015$$

Xem xÃ©t thuá»™c tÃ­nh **Weekend** {yes, no}

- **Weekend** = yes

$$Info([5, 2]) = Entropy([5, 2]) = -\frac{5}{7}log_2\left(\frac{5}{7}\right) -\frac{2}{7}log_2\left(\frac{2}{7}\right) = 0.86312$$

- **Weekend** = no

$$Info([1, 2]) = Entropy([1, 2]) = -\frac{1}{3}log_2\left(\frac{1}{3}\right) -\frac{2}{3}log_2\left(\frac{2}{3}\right) = 0.918296$$

- ThÃ´ng tin cá»§a thuá»™c tÃ­nh **Weekend**

$$Info([5, 2], [1, 2]) = \frac{7}{10} \times 0.86312 + \frac{3}{10} \times 0.918296 = 0.879673$$

- Äá»™ lá»£i thÃ´ng tin cá»§a thuá»™c tÃ­nh **Weekend**

$$Info([6, 4]) - Info([5, 2], [1, 2]) = Entropy(S) - Info([5, 2], [1, 2]) = 0.09128$$

Xem xÃ©t thuá»™c tÃ­nh **Sun** {yes, no}

- **Sun** = yes

$$Info([5, 3]) = Entropy([5, 3]) = -\frac{5}{8}log_2\left(\frac{5}{8}\right) -\frac{3}{8}log_2\left(\frac{3}{8}\right) = 0.95443$$

- **Sun** = no

$$Info([1, 1]) = Entropy([1, 1]) = -\frac{1}{2}log_2\left(\frac{1}{2}\right) -\frac{1}{2}log_2\left(\frac{1}{2}\right) = 1$$

- ThÃ´ng tin cá»§a thuá»™c tÃ­nh **Sun**

$$Info([5, 3], [1, 1]) = \frac{8}{10} \times  0.95443 + \frac{2}{10} \times 1 = 0.963544$$

- Äá»™ lá»£i thÃ´ng tin cá»§a thuá»™c tÃ­nh **Sun**

$$Info([6, 4]) - Info([5, 3], [1, 1]) = Entropy(S) - Info([5, 3], [1, 1]) = 7.406 \times 10^{-3}$$

Chá»n thuá»™c tÃ­nh **Snow\_Dist** do cÃ³ Information Gain cao nháº¥t

## CÃ¢u 06: Naive Bayes

TÃ¬m mÃ´ hÃ¬nh **naÃ¯ve bayes** (khÃ´ng cáº§n cháº¡y tá»«ng bÆ°á»›c)

MÃ´ hÃ¬nh Naive Bayes

- P(Skiing)

|  	|   	|
|---	|---	|
| yes  	|  6/10 	|
| no  	|  4/10 	|

- P(Snow_Dist \| Skiing)

|   	|   	|  Snow_Dist 	|  Snow_Dist 	|
|---	|---	|---	|---	|
|   	|   	|   $\leq 100$ 	|  $> 100$ 	|
| Skiing  	|  yes 	|  4/6 	|  2/6 	|
| Skiing  	|  no 	|  0/4 	|  4/4 	|


- P(Weekend \| Skiing)

|   	|   	|  Snow_Dist 	|  Snow_Dist 	|
|---	|---	|---	|---	|
|   	|   	|   yes 	|  no 	|
| Skiing  	|  yes 	|  5/6 	|  1/6 	|
| Skiing  	|  no 	|  2/4 	|  2/4 	|


- P(Sun \| Skiing)

|   	|   	|  Snow_Dist 	|  Snow_Dist 	|
|---	|---	|---	|---	|
|   	|   	|   yes 	|  no 	|
| Skiing  	|  yes 	|  5/6 	|  1/6 	|
| Skiing  	|  no 	|  3/4 	|  1/4 	|

# Máº¡ng Neural network 

## CÃ¢u 07: Convert expression to computational graph
Cho biá»ƒu thá»©c $y = (ax+b)(cx+d) + sin(c+d)$ hÃ£y chuyá»ƒn biá»ƒu thá»©c thÃ nh Ä‘á»“ thá»‹ tÃ­nh toÃ¡n vÃ 
váº½ Ä‘á»“ thá»‹ nÃ y

**HÆ°á»›ng Ä‘i**: MÃ¬nh nghÄ© váº½ ra cÅ©ng dá»… :))))


## CÃ¢u 08: Caluate on Computational Graph

|![](/assets/images_posts/com_graph_00.png)|
|:--:| 
| |

TÃ­nh giÃ¡ trá»‹ biáº¿n ouput $y$ náº¿u cÃ¡c biáº¿n input cÃ³ giÃ¡ trá»‹ $x_1$ = 3, $x_2$ = âˆ’2

**HÆ°á»›ng Ä‘i**

Ta cÃ³: $x_1 = 3$, $x_2 = âˆ’2$, $w_1 = 3$, $w_2 = 4$

$$a \leftarrow w_1 \times x_1 = 3 \times 3 = 9$$

$$b \leftarrow w_2 \times x_2 = -2 \times 4 = -8$$

$$c \leftarrow a \times b = 9 \times -8 = -72$$

$$y \leftarrow \sigma(c) = \frac{1}{1 + exp(c)} = \frac{1}{1 + exp(-72)} = 1$$


## CÃ¢u 09: Derivatives with Computational Graph
TÃ­nh toÃ¡n Ä‘áº¡o hÃ m riÃªng 

$$\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \frac{\partial y}{\partial w_1}, \frac{\partial y}{\partial w_2}$$

**HÆ°á»›ng Ä‘i**

$$\frac{\partial y}{\partial x_1} = \frac{\partial y}{\partial c}\frac{\partial c}{\partial a}\frac{\partial a}{\partial x_1} = \sigma(c)\left[1-\sigma(c)\right]bw_1$$

$$\frac{\partial y}{\partial x_2} = \frac{\partial y}{\partial c}\frac{\partial c}{\partial b}\frac{\partial b}{\partial x_2} = \sigma(c)\left[1-\sigma(c)\right]aw_2$$

$$\frac{\partial y}{\partial w_1} = \frac{\partial y}{\partial c}\frac{\partial c}{\partial a}\frac{\partial a}{\partial w_1} = \sigma(c)\left[1-\sigma(c)\right]bx_1$$

$$\frac{\partial y}{\partial w_2} = \frac{\partial y}{\partial c}\frac{\partial c}{\partial b}\frac{\partial b}{\partial w_2} = \sigma(c)\left[1-\sigma(c)\right]ax_2$$
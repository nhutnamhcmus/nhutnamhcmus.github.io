---
title: "Gi·∫£i ƒë·ªÅ m·∫´u Nh·∫≠p m√¥n H·ªçc M√°y"
categories:
  - Exam
  - Machine Learning
tags:
  - Exam
  - Machine Learning
toc: true
comments: true
---

ƒê·ªÅ thi m·∫´u m√¥n Nh·∫≠p m√¥n H·ªçc M√°y - K2018

# M√¥ h√¨nh tuy·∫øn t√≠nh

## C√¢u 01:
Cho m√¥ h√¨nh perceptron $y=sign(w_0 + w_1x)$ v·ªõi $w_0 = 1$, $w_1 = 1$ v√† b·ªô d·ªØ li·ªáu ki·ªÉm tra $D = {(x_i, y_i)} = {(2, -1), (3, 1), (-2, -1), (-4, 1)}$. H√£y t√≠nh **ƒë·ªô ch√≠nh x√°c** c·ªßa m√¥ h√¨nh?

**H∆∞·ªõng ƒëi**

G·ªçi bi·∫øn to√†n c·ª•c correct = 0 l√† s·ªë l·∫ßn d·ª± ƒëo√°n ƒë√∫ng c·ªßa m√¥ h√¨nh

V·ªõi $x_1 = 2$, ta c√≥ $y_{\text{pred}} = sign(1 + 2 \times 1) = 1 \ne y_1 = -1$

V·ªõi $x_2 = 3$, ta c√≥ $y_{\text{pred}} = sign(1 + 3 \times 1) = 1 = y_2 = 1$, correct += 1

V·ªõi $x_3 = -2$, ta c√≥ $y_{\text{pred}} = sign(1 + (-2) \times 1) = -1 = y_3 = -1$, correct += 1

V·ªõi $x_4 = -4$, ta c√≥ $y_{\text{pred}} = sign(1 + (-4) \times 1) = -1 \ne y_4 = 1$

ƒê·ªô ch√≠nh x√°c m√¥ h√¨nh

$$\text{accuracy} = \frac{1}{n}\sum_{i=1}^{n}\left[sign(w_0 + w_1x_i) == y_i\right] = \frac{1}{4} \times 2 = 0.5$$


## C√¢u 02:
Cho m√¥ h√¨nh logistic regression:
$$y = \frac{1}{1+ exp(w_0 + w_1x)}$$ v·ªõi $w_0 = 1$, $w_1 = 1$ v√† b·ªô d·ªØ li·ªáu ki·ªÉm tra $D = {(x_i, y_i)} = {(-3, -1), (-2, 1), (2, -1), (4, 1)}$. H√£y t√≠nh **ƒë·ªô l·ªói** c·ªßa m√¥ h√¨nh?

**H∆∞·ªõng ƒëi**

G·ªçi bi·∫øn to√†n c·ª•c error = 0 l√† s·ªë l·∫ßn d·ª± ƒëo√°n b·ªã sai c·ªßa m√¥ h√¨nh

V·ªõi $x_1 = -3$, ta c√≥ $y_{\text{pred}} = \left[\frac{1}{1+ exp(1 + 1(-3))} >= 0.5\right] = 1 \ne y_1 = -1$, error += 1

V·ªõi $x_2 = -2$, ta c√≥ $y_{\text{pred}} = \left[\frac{1}{1+ exp(1 + 1(-2))} >= 0.5\right] = 1 = y_2 = 1$

V·ªõi $x_3 = 2$, ta c√≥ $y_{\text{pred}} = \left[\frac{1}{1+ exp(1 + 1(2))} >= 0.5\right] = -1 = y_3 = -1$

V·ªõi $x_4 = 4$, ta c√≥ $y_{\text{pred}} = \left[\frac{1}{1+ exp(1 + 1(4))} >= 0.5\right] = -1 \ne y_4 = 1$, error += 1

ƒê·ªô l·ªói c·ªßa m√¥ h√¨nh 

$$\text{error rate} = \frac{1}{n}\sum_{i=1}^{n}\left[\frac{1}{1+ exp(w_0 + w_1x_i)} \ne y_i\right] = \frac{1}{4} \times 2 = 0.5 = 1 - \text{accuracy}$$

## C√¢u 03:
Cho m√¥ h√¨nh linear regression:
$$y = f(x) = w_0 + w_1x$$ v√† b·ªô d·ªØ li·ªáu D, h√£y x√°c ƒë·ªãnh m√¥ h√¨nh v√† tr·ª±c quan m√¥ h√¨nh

|  x 	|   y	|
|---	|---	|
|   1	|   2	|
|   2	|   1	|
|   4	|   2	|

$$ X = 
\begin{bmatrix}
    1       & 1\\
    1       & 2\\
    1       & 4
\end{bmatrix}
$$

$$ y = 
\begin{bmatrix}
    2 \\
    1 \\
    2 
\end{bmatrix}
$$

T√≠nh ma tr·∫≠n chuy·ªÉn v·ªã c·ªßa X 

$$ X^T = 
\begin{bmatrix}
    1 & 1 & 1\\
    1 & 2 & 4
\end{bmatrix}
$$

M√¥ h√¨nh t·ªët nh·∫•t

$$W = \left(X^TX\right)^{-1}X^Ty$$

```python
import numpy as np 
import matplotlib.pyplot as plt 

xs = np.array([1, 2, 4])
ys = np.array([2, 1, 2])

one = np.ones(xs.shape[0])
X = np.stack((one, xs)).T

W = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), ys)
print(W)

[Output] [1.5        0.07142857]

x = np.linspace(1,4)
y_predict = W[0] + W[1]*x 
plt.scatter(xs, ys)
plt.plot(x, y_predict, c='red')
plt.show()
```

|![](/assets/images_posts/ml-testt.png)|


# M√¥ h√¨nh c√¢y quy·∫øt ƒë·ªãnh v√† th·ªëng k√™

D·ªØ li·ªáu hu·∫•n luy·ªán b·∫£ng sau ùê∑ c√≥ 3 thu·ªôc t√≠nh Snow\_Dist, Weekend, Sun v√† m·ªôt thu·ªôc t√≠nh quy·∫øt
ƒë·ªãnh Skiing. C√¢u 4, 5 v√† 6 s·∫Ω s·ª≠ d·ª•ng d·ªØ li·ªáu n√†y.

## C√¢u 04:
T√¨m v√† v·∫Ω t·∫•t c·∫£ c√°c c√¢y stump s·ª≠ d·ª•ng ƒë·ªô ƒëo **gini** (kh√¥ng c·∫ßn ch·∫°y t·ª´ng b∆∞·ªõc)

## C√¢u 05: 
T√¨m v√† v·∫Ω c√¢y quy·∫øt ƒë·ªãnh s·ª≠ d·ª•ng ƒë·ªô ƒëo **entropy** (kh√¥ng c·∫ßn ch·∫°y t·ª´ng b∆∞·ªõc)

## C√¢u 06:
T√¨m m√¥ h√¨nh **na√Øve bayes** (kh√¥ng c·∫ßn ch·∫°y t·ª´ng b∆∞·ªõc)

# M·∫°ng Neural network 

## C√¢u 07:
Cho bi·ªÉu th·ª©c $y = (ax+b)(cx+d) + sin(c+d)$ h√£y chuy·ªÉn bi·ªÉu th·ª©c th√†nh ƒë·ªì th·ªã t√≠nh to√°n v√†
v·∫Ω ƒë·ªì th·ªã n√†y

**H∆∞·ªõng ƒëi**: M√¨nh nghƒ© v·∫Ω ra c≈©ng d·ªÖ :))))


## C√¢u 08:

|![](/assets/images_posts/com_graph_00.png)|
|:--:| 
| |

T√≠nh gi√° tr·ªã bi·∫øn ouput $y$ n·∫øu c√°c bi·∫øn input c√≥ gi√° tr·ªã $x_1$ = 3, $x_2$ = ‚àí2

**H∆∞·ªõng ƒëi**

Ta c√≥: $x_1 = 3$, $x_2 = ‚àí2$, $w_1 = 3$, $w_2 = 4$

$$a \leftarrow w_1 \times x_1 = 3 \times 3 = 9$$

$$b \leftarrow w_2 \times x_2 = -2 \times 4 = -8$$

$$c \leftarrow a \times b = 9 \times -8 = -72$$

$$y \leftarrow \sigma(c) = \frac{1}{1 + exp(c)} = \frac{1}{1 + exp(-72)} = 1$$


## C√¢u 09:
T√≠nh to√°n ƒë·∫°o h√†m ri√™ng 

$$\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \frac{\partial y}{\partial w_1}, \frac{\partial y}{\partial w_2}$$

**H∆∞·ªõng ƒëi**

$$\frac{\partial y}{\partial x_1} = \frac{\partial y}{\partial c}\frac{\partial c}{\partial a}\frac{\partial a}{\partial x_1} = \sigma(c)\left[1-\sigma(c)\right]bw_1$$

$$\frac{\partial y}{\partial x_2} = \frac{\partial y}{\partial c}\frac{\partial c}{\partial b}\frac{\partial b}{\partial x_2} = \sigma(c)\left[1-\sigma(c)\right]aw_2$$

$$\frac{\partial y}{\partial w_1} = \frac{\partial y}{\partial c}\frac{\partial c}{\partial a}\frac{\partial a}{\partial w_1} = \sigma(c)\left[1-\sigma(c)\right]bx_1$$

$$\frac{\partial y}{\partial w_2} = \frac{\partial y}{\partial c}\frac{\partial c}{\partial b}\frac{\partial b}{\partial w_2} = \sigma(c)\left[1-\sigma(c)\right]ax_2$$